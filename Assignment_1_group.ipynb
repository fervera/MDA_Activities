{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a338480c-b232-4a89-97ce-12d7fbd2e7b9",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\">Table of Contents</h1>\n",
    "\n",
    "<ol>\n",
    "  <li><a href=\"#step1\">Step 1 – Load the Dataset</a></li>\n",
    "  <li><a href=\"#step2\">Step 2 – Remove Columns with Excessive Missing Data</a></li>\n",
    "  <li><a href=\"#step3\">Step 3 – Remove Irrelevant Columns</a></li>\n",
    "  <li><a href=\"#step4\">Step 4 – Verify the Cleaned Dataset</a></li>\n",
    "  <li><a href=\"#step5\">Step 5 – Dropping Rows with Missing Values</a></li>\n",
    "  <li><a href=\"#step6\">Step 6 – Checking for Inconsistencies</a></li>\n",
    "  <li><a href=\"#step7\">Step 7 – Aggregate the Data</a></li>\n",
    "  <li><a href=\"#summary8\">Step 8 - Final Summary of the Project</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a96b78",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\">Step 1 – Load the Dataset</h1>\n",
    "\n",
    "<h3>Objective</h3>\n",
    "\n",
    "<p>\n",
    "The goal of this step is to import the <b>BRFSS 2024 dataset</b> (in SAS XPT format) into a pandas DataFrame and inspect its structure.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8eb5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries successfully imported.\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display confirmation message\n",
    "print(\"Libraries successfully imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be9139",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">1.1 Import the BRFSS 2024 Dataset</h2>\n",
    "\n",
    "<p>\n",
    "We will load the <b>BRFSS 2024 (LLCP2024.XPT)</b> file, which is provided in SAS Transport format.<br>\n",
    "The goal is to confirm that the file loads correctly and inspect its shape and columns.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8844f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SAS XPT dataset\n",
    "df = pd.read_sas(\"LLCP2024.XPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcad644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (457670, 301)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape (rows, columns)\n",
    "print(\"Dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few column names in a DataFrame for better readability\n",
    "columns_df = pd.DataFrame(df.columns, columns=[\"Column Names\"])\n",
    "columns_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa844d",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\">Step 2 – Remove Columns with Excessive Missing Data</h1>\n",
    "\n",
    "We will check for duplicates and drop all columns with more than **15,000** missing values.  \n",
    "A simple log of removed columns will be created for documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08cb4ae",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">2.1 Check and Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "n_duplicates = df.duplicated().sum()\n",
    "print(f\"Found {n_duplicates} duplicate rows.\")\n",
    "\n",
    "# Remove duplicates and reset index\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "print(\"Shape after removing duplicates:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a75df9c",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">2.2 Identify Columns with Excessive Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ab4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "missing_summary = (\n",
    "    df.isnull()\n",
    "      .sum()\n",
    "      .to_frame(name=\"MissingCount\")\n",
    "      .assign(MissingPercent=lambda x: (x[\"MissingCount\"] / len(df) * 100).round(2))\n",
    "      .sort_values(\"MissingCount\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Top 20 columns with the highest missing values:\")\n",
    "missing_summary.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efee803",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">2.3 Drop Columns with More Than 15,000 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold\n",
    "THRESHOLD = 15000\n",
    "\n",
    "# Identify columns exceeding the threshold\n",
    "cols_to_drop = missing_summary[missing_summary[\"MissingCount\"] > THRESHOLD].index.tolist()\n",
    "print(f\"Columns exceeding {THRESHOLD} missing values: {len(cols_to_drop)}\")\n",
    "\n",
    "# Drop the columns\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "print(\"Shape after dropping columns:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be42858",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">2.4 Document Removed Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc875ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documentation table for dropped columns\n",
    "removed_cols_doc = missing_summary.loc[cols_to_drop].reset_index()\n",
    "removed_cols_doc.columns = [\"Column\", \"MissingCount\", \"MissingPercent\"]\n",
    "\n",
    "removed_cols_doc.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dfcd1",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\">Step 3 – Remove Irrelevant Columns</h1>\n",
    "\n",
    "<p>\n",
    "In this step, we will manually remove columns that are not relevant for the analysis.  \n",
    "These variables are either administrative, redundant, or unrelated to the project’s objectives.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of irrelevant columns to drop\n",
    "irrelevant_columns = [\n",
    "    \"FMONTH\", \"IDATE\", \"IMONTH\", \"IDAY\", \"IYEAR\", \"DISPCODE\", \"SEQNO\", \"_PSU\",\n",
    "    \"SEXVAR\", \"PERSDOC3\", \"MEDCOST1\", \"RMVTETH4\", \"CPDEMO1C\", \"VETERAN3\",\n",
    "    \"QSTVER\", \"QSTLANG\", \"_STSTR\", \"_STRWT\", \"_RAWRAKE\", \"_WT2RAKE\", \"_IMPRACE\",\n",
    "    \"_DUALUSE\", \"_LLCPWT2\", \"_LLCPWT\", \"_RFHLTH\", \"_PHYS14D\", \"_MENT14D\",\n",
    "    \"_HLTHPL2\", \"_HCVU654\", \"_TOTINDA\", \"_EXTETH3\", \"_DENVST3\", \"_MICHD\",\n",
    "    \"_LTASTH1\", \"_CASTHM1\", \"_ASTHMS1\", \"_DRDXAR2\", \"_MRACE1\", \"_HISPANC\",\n",
    "    \"_RACEG21\", \"_RACEGR3\", \"_RACEPRV\", \"_AGEG5YR\", \"_AGE65YR\", \"_AGE_G\",\n",
    "    \"_RFBMI5\", \"_CHLDCNT\", \"_EDUCAG\", \"_INCOMG1\", \"_SMOKER3\", \"_RFSMOK3\",\n",
    "    \"_CURECI3\", \"_LCSAGE\", \"DRNKANY6\", \"DROCDY4_\", \"_RFBING6\", \"_DRNKWK3\",\n",
    "    \"_RFDRHV9\", \"_METSTAT\", \"_URBSTAT\"\n",
    "]\n",
    "\n",
    "# Check which of these columns exist in the dataset\n",
    "existing_cols = [col for col in irrelevant_columns if col in df.columns]\n",
    "\n",
    "print(f\"Total columns to drop (found in dataset): {len(existing_cols)}\")\n",
    "\n",
    "# Drop them\n",
    "df = df.drop(columns=existing_cols)\n",
    "print(\"Shape after dropping irrelevant columns:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c1b60",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\">Step 4 – Verify the Cleaned Dataset</h1>\n",
    "\n",
    "<p>\n",
    "In this step, we will verify the structure and integrity of the cleaned dataset after removing irrelevant and incomplete columns.  \n",
    "The goal is to ensure that the DataFrame now contains only valid, usable, and relevant variables for analysis.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290713e",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">4.1 Print the New Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new shape of the cleaned dataset\n",
    "print(\" New dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d960d",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">4.2 Generate a List of Remaining Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to visualize the first 30 remaining columns\n",
    "remaining_cols_df = pd.DataFrame(df.columns, columns=[\"Remaining Columns\"])\n",
    "remaining_cols_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a7619",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">4.3 Confirm No Irrelevant or Excessively Incomplete Columns Remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77318801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck missing values after cleaning\n",
    "remaining_missing = (\n",
    "    df.isnull()\n",
    "      .sum()\n",
    "      .to_frame(name=\"MissingCount\")\n",
    "      .assign(MissingPercent=lambda x: (x[\"MissingCount\"] / len(df) * 100).round(2))\n",
    "      .sort_values(\"MissingCount\", ascending=False)\n",
    ")\n",
    "\n",
    "# Display top 10 columns by missing percentage\n",
    "remaining_missing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3589e5e-3a5f-4bcf-950f-d4bed136c118",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\">Step 5 – Dropping Rows with Missing Values</h1>\n",
    "\n",
    "<p>\n",
    "In this step, we examined the percentage of missing data in each column and determined an appropriate cleaning approach.  \n",
    "Columns with a missing rate of less than <b>0.01%</b> were considered statistically insignificant, and the rows containing those missing values were safely <b>dropped</b> using the <b>.dropna()</b> function.  \n",
    "However, important variables such as <b>HEIGHT3</b>, <b>WEIGHT2</b>, <b>INCOME3</b>, <b>CHILDREN</b>, and <b>EMPLOY1</b> were retained for <b>imputation</b>, as they represent key demographic and health indicators.  \n",
    "This selective approach minimizes unnecessary data loss while ensuring the dataset remains accurate, consistent, and representative for further analysis.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281af4dd-de0b-4df3-8377-52a36c4e9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': df.isnull().sum(),\n",
    "    'Missing %': round((df.isnull().sum() / len(df)) * 100, 3)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing Count'] > 0].sort_values(by='Missing %', ascending=False)\n",
    "missing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04acae5-6ca9-4caf-8a1f-194ceb197bf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: List columns we will keep for imputation\n",
    "impute_cols = ['HEIGHT3', 'WEIGHT2', 'INCOME3', 'CHILDREN', 'EMPLOY1']\n",
    "\n",
    "# Step 2: Drop rows that have missing values in OTHER columns (less than 0.01%)\n",
    "df = df.dropna(subset=[col for col in df.columns if col not in impute_cols])\n",
    "\n",
    "# Step 2.5: Create an independent copy to avoid SettingWithCopyWarning\n",
    "df = df.copy()\n",
    "\n",
    "# Step 3: Verify dataset shape after dropping\n",
    "print(\"Dataset shape after dropping negligible missing rows:\", df.shape)\n",
    "\n",
    "# Step 4: Check remaining missingness (should only be in the 5 selected columns)\n",
    "missing_summary = (df[impute_cols].isnull().sum() / len(df)) * 100\n",
    "print(\"\\nRemaining columns to impute and their missing percentages:\\n\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac2ced-613d-470f-a163-9adf6c354189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we plan to impute\n",
    "impute_cols = ['HEIGHT3', 'WEIGHT2', 'INCOME3', 'CHILDREN', 'EMPLOY1']\n",
    "\n",
    "# Display datatypes\n",
    "df[impute_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb1160-a678-40f8-b090-146458645ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we plan to analyze\n",
    "impute_cols = ['HEIGHT3', 'WEIGHT2', 'INCOME3', 'CHILDREN', 'EMPLOY1']\n",
    "\n",
    "# Loop through and print unique values for each\n",
    "for col in impute_cols:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(df[col].unique())\n",
    "    print(f\"Total unique values: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d1c4a-f546-4d98-80ff-a6a49cbdbeae",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">5.1 Imputing Missing Values for HEIGHT3</h2>\n",
    "\n",
    "<p>\n",
    "The variable <b>HEIGHT3</b> captures respondents’ self-reported height, primarily recorded in feet and inches (ranging from <b>200</b> to <b>711</b>).  \n",
    "According to the dataset documentation, several special codes were used to represent non-responses or alternative formats:  \n",
    "<ul>\n",
    "  <li><b>7777</b> – “Don’t know / Not sure”</li>\n",
    "  <li><b>9999</b> – “Refused”</li>\n",
    "  <li><b>9061–9998</b> – Metric height responses (meters/centimeters)</li>\n",
    "  <li><b>Blank</b> – “Not asked or Missing”</li>\n",
    "</ul>\n",
    "These coded responses do not represent valid measurements and were therefore replaced with <b>NaN</b> values.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Since height is a continuous numeric variable, missing values were imputed using the <b>median height grouped by sex</b>.  \n",
    "This group-based approach maintains the natural differences in average height between male and female respondents, while avoiding the influence of extreme values.  \n",
    "Metric-coded entries (values beginning with <b>9</b>) were rare and were treated as missing to ensure consistent units across the dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d97ad0-dce8-4760-bf2a-ea5ed0da4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Safely replace invalid codes with NaN using .loc to avoid the warning\n",
    "df.loc[:, 'HEIGHT3'] = df['HEIGHT3'].replace([7777, 9999], np.nan)\n",
    "\n",
    "# Confirm changes\n",
    "print(\"Missing after recoding:\", df['HEIGHT3'].isnull().sum())\n",
    "\n",
    "# Display a few valid values\n",
    "print(\"\\nUnique value sample after cleaning:\")\n",
    "print(sorted(df['HEIGHT3'].dropna().unique())[:20])\n",
    "\n",
    "# Save value before imputation\n",
    "height3_missing_before_imputation = df['HEIGHT3'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c6db4-38e5-4e8d-8993-79ec814d0199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Treat metric-coded entries (9061–9998) as missing to keep one unit system\n",
    "metric_mask = (df['HEIGHT3'] >= 9061) & (df['HEIGHT3'] <= 9998)\n",
    "df.loc[metric_mask, 'HEIGHT3'] = np.nan\n",
    "\n",
    "# 2) Impute HEIGHT3 with the median by sex\n",
    "#    BRFSS: _SEX is coded (1 = Male, 2 = Female). Your _SEX had 0% missing earlier.\n",
    "df['HEIGHT3'] = df['HEIGHT3'].fillna(\n",
    "    df.groupby('_SEX')['HEIGHT3'].transform('median')\n",
    ")\n",
    "\n",
    "# 3) Verify imputation\n",
    "print(\"Remaining missing in HEIGHT3:\", df['HEIGHT3'].isna().sum())\n",
    "print(\"Median by sex used for imputation:\")\n",
    "print(df.groupby('_SEX')['HEIGHT3'].median())\n",
    "\n",
    "# 4) (Optional) Create a human-friendly inches variable for analysis/viz\n",
    "#    HEIGHT3 codes like 504 mean 5 feet 04 inches → convert to inches\n",
    "def code_to_inches(code):\n",
    "    feet = int(code // 100)\n",
    "    inches = int(code % 100)\n",
    "    return feet * 12 + inches\n",
    "\n",
    "df['HEIGHT3_in'] = df['HEIGHT3'].apply(code_to_inches)\n",
    "\n",
    "# Quick sanity check\n",
    "df[['HEIGHT3', 'HEIGHT3_in']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f8631-46ab-4530-baed-576bf8f5f501",
   "metadata": {},
   "source": [
    "Since <b>HEIGHT3</b> is a continuous numeric variable representing physical height, missing values were imputed using the <b>median height grouped by sex</b>. This approach maintains the biological differences in average height between male and female respondents and minimizes bias from extreme values. The computed median heights were <b>510.0</b> for males and <b>504.0</b> for females, expressed in BRFSS format (feet–inches concatenated). After imputation, all missing entries were successfully filled, resulting in <b>0 remaining missing values</b>. </p> <p> Following imputation, a new column named <b>HEIGHT3_in</b> was created to convert the coded height values into actual inches. </p>\n",
    "<p> This transformation allows easier analysis and statistical comparison by standardizing the unit of measurement. By converting <b>HEIGHT3</b> into inches, the dataset now supports consistent numeric analysis, accurate BMI computation, and easier integration with other continuous health metrics in later stages of the project. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be2dc5-68ac-42fb-9680-bd1bae61fcfc",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">5.2 Imputing Missing Values for WEIGHT2</h2>\n",
    "\n",
    "<p>\n",
    "The variable <b>WEIGHT2</b> records respondents’ self-reported body weight without shoes, primarily measured in pounds.  \n",
    "According to the BRFSS 2024 data dictionary, the variable includes multiple coded values representing invalid or non-response entries:  \n",
    "<ul>\n",
    "  <li><b>7777</b> – “Don’t know / Not sure”</li>\n",
    "  <li><b>9999</b> – “Refused”</li>\n",
    "  <li><b>9023–9352</b> – Metric weight responses (kilograms)</li>\n",
    "  <li><b>Blank</b> – “Not asked or Missing”</li>\n",
    "</ul>\n",
    "All of these codes were replaced with <b>NaN</b> to ensure that only valid weight observations were retained for further analysis.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Since <b>WEIGHT2</b> is a continuous numeric variable and directly affects downstream analyses such as BMI calculation, missing values were imputed using the <b>median weight grouped by sex</b>.  \n",
    "This approach preserves the natural differences between male and female weight distributions while preventing distortion caused by extreme outliers.  \n",
    "Metric-coded responses (values greater than <b>9000</b>) were rare and were also treated as missing to maintain consistent units.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ddab7-3be9-429e-859a-f3fd9d82c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- STEP 1: Copy the column for safety ---\n",
    "w = df[\"WEIGHT2\"].copy()\n",
    "\n",
    "# --- STEP 2: Define invalid or special codes ---\n",
    "invalid_codes = {7777, 9999}       # \"Don't know\" / \"Refused\"\n",
    "metric_low, metric_high = 9023, 9352  # Metric-coded responses (in kilograms)\n",
    "\n",
    "# --- STEP 3: Replace invalid and metric-coded values with NaN ---\n",
    "w = w.mask(w.isin(invalid_codes), np.nan)                      # replace 7777, 9999\n",
    "w = w.mask((w >= metric_low) & (w <= metric_high), np.nan)     # replace 9023–9352\n",
    "\n",
    "# --- STEP 4: Optional range cleanup (keep plausible values only, 50–776 lbs) ---\n",
    "w = w.mask((w < 50) | (w > 776), np.nan)\n",
    "\n",
    "# --- STEP 5: Save the cleaned version back into the DataFrame ---\n",
    "df[\"WEIGHT2\"] = w\n",
    "df[\"WEIGHT_lb\"] = w  # new clear column for explicit unit (pounds)\n",
    "\n",
    "# Save value before imputation\n",
    "weight2_missing_before_imputation = df[\"WEIGHT2\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e334a-c589-49cc-8257-9f545fe41252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 6: Calculate median weights by sex (_SEX: 1 = Male, 2 = Female) ---\n",
    "medians_by_sex = df.groupby(\"_SEX\")[\"WEIGHT2\"].median()\n",
    "print(\"Median weights by sex (lbs):\\n\", medians_by_sex)\n",
    "\n",
    "# --- STEP 7: Fill missing values using each group’s median ---\n",
    "df[\"WEIGHT2\"] = df[\"WEIGHT2\"].fillna(df[\"_SEX\"].map(medians_by_sex))\n",
    "df[\"WEIGHT_lb\"] = df[\"WEIGHT2\"]  # keep synchronized\n",
    "\n",
    "# --- STEP 8: Verify results ---\n",
    "weight2_missing_after_imputation = df[\"WEIGHT2\"].isna().sum()\n",
    "print(\"Remaining missing in WEIGHT2:\", weight2_missing_after_imputation)\n",
    "print(\"Post-imputation WEIGHT2 range (lbs):\", df[\"WEIGHT2\"].min(), \"to\", df[\"WEIGHT2\"].max())\n",
    "\n",
    "# Summary of WEIGHT2 imputation\n",
    "df.loc[:, [\"_SEX\", \"WEIGHT2\", \"WEIGHT_lb\"]].head(10)\n",
    "\n",
    "weight2_values_imputed = weight2_missing_before_imputation - weight2_missing_after_imputation\n",
    "print(f\"\\nWEIGHT2 Imputation Summary:\")\n",
    "print(f\"Values imputed: {weight2_values_imputed:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1e32f-8b38-4b4c-a684-09832cbdf982",
   "metadata": {},
   "source": [
    "The calculated median weights were <b>194 lbs</b> for males (<code>_SEX = 1</code>) and <b>160 lbs</b> for females (<code>_SEX = 2</code>).  \n",
    "All missing values were successfully imputed, resulting in <b>0 remaining missing entries</b> in the dataset.  \n",
    "The valid weight range after cleaning and imputation extended from <b>50 lbs</b> to <b>776 lbs</b>, consistent with the official BRFSS 2024 codebook.\n",
    "</p>\n",
    "The resulting <b>WEIGHT_lb</b> column now contains consistent and realistic weight values for all respondents, ready for use in <b>BMI</b> computation and subsequent health-related analyses.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1b55b-68c1-4878-bde9-d13c8f50dfe8",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">5.3 Imputing Missing values for INCOME3</h2>\n",
    "\n",
    "<p>\n",
    "The variable <b>INCOME3</b> captures respondents’ annual household income from all sources, coded into 11 ordinal categories.  \n",
    "Each numeric code corresponds to a specific income range, while additional codes represent missing or invalid responses.  \n",
    "According to the BRFSS codebook:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li><b>1–11:</b> Valid income categories ranging from \"$10,000 to 200,000\" or more</li>\n",
    "  <li><b>77:</b> “Don’t know / Not sure”</li>\n",
    "  <li><b>99:</b> “Refused”</li>\n",
    "  <li><b>Blank:</b> “Not asked or Missing”</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "Values coded as <b>77</b> or <b>99</b> do not represent valid numeric income categories and were replaced with <b>NaN</b> values to avoid bias in downstream analysis.  \n",
    "Since income is an ordinal categorical variable, imputation was not appropriate; instead, these missing responses were maintained as <b>NaN</b> to preserve data integrity.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bce0f-55a7-46b3-a217-e5cc5817c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: Copy column for safety ---\n",
    "inc = df[\"INCOME3\"].copy()\n",
    "\n",
    "# --- STEP 2: Replace invalid and missing codes with NaN ---\n",
    "invalid_codes = [77, 99]\n",
    "inc = inc.mask(inc.isin(invalid_codes), np.nan)\n",
    "\n",
    "# --- STEP 3: Save cleaned column ---\n",
    "df[\"INCOME3\"] = inc\n",
    "\n",
    "print(\"Unique values after cleaning:\", sorted(df[\"INCOME3\"].dropna().unique()))\n",
    "print(\"Missing INCOME3 values:\", df[\"INCOME3\"].isna().sum())\n",
    "\n",
    "# Save value before imputation\n",
    "income3_missing_before_imputation = df[\"INCOME3\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95519c53-1cb4-4687-a513-76cafd94bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ratio = df[\"INCOME3\"].isna().mean() * 100\n",
    "missing_income3 = df[\"INCOME3\"].isna().sum()\n",
    "print(f\"Missing ratio: {missing_ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_labels = {\n",
    "    1: \"Less than $10,000\",\n",
    "    2: \"$10,000 to < $15,000\",\n",
    "    3: \"$15,000 to < $20,000\",\n",
    "    4: \"$20,000 to < $25,000\",\n",
    "    5: \"$25,000 to < $35,000\",\n",
    "    6: \"$35,000 to < $50,000\",\n",
    "    7: \"$50,000 to < $75,000\",\n",
    "    8: \"$75,000 to < $100,000\",\n",
    "    9: \"$100,000 to < $150,000\",\n",
    "    10: \"$150,000 to < $200,000\",\n",
    "    11: \"$200,000 or more\"\n",
    "}\n",
    "\n",
    "df[\"Income_Level\"] = df[\"INCOME3\"].map(income_labels)\n",
    "df[\"Income_Level\"] = df[\"Income_Level\"].fillna(\"Unknown\")\n",
    "\n",
    "print(df[\"Income_Level\"].value_counts(dropna=False).head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e90d3-19ea-4c72-93eb-c2d3cb8ea3ab",
   "metadata": {},
   "source": [
    "Unlike <b>HEIGHT3</b> and <b>WEIGHT2</b>, we decided not to impute missing values for <b>INCOME3</b>.  \n",
    "Income is an <b>ordinal variable</b> where people often skip the question or choose “Don’t know” or “Refused,” which usually isn’t random.  \n",
    "These missing values can be related to factors like age, education, or employment, so filling them in with averages could easily introduce bias.\n",
    "</p>\n",
    "<p>\n",
    "Instead, we kept all missing and invalid responses as a separate category called <b>\"Unknown\"</b>.  \n",
    "Overall, this approach keeps the dataset realistic and avoids making assumptions about respondents’ income levels.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acf6af-04ac-4d9d-9e53-33127b40ca81",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">5.4 Imputing Missing values for CHILDREN</h2>\n",
    "\n",
    "<p>\n",
    "The variable <b>CHILDREN</b> represents the number of people under 18 years old living in each respondent’s household.  \n",
    "According to the BRFSS 2024 codebook, it’s a numeric field with a few special codes used to handle nonstandard answers.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li><b>1–87</b> → Actual number of children reported.</li>\n",
    "  <li><b>88</b> → “None,” meaning the respondent has no children under 18 at home.</li>\n",
    "  <li><b>99</b> → “Refused” to answer.</li>\n",
    "  <li><b>Blank</b> → “Not asked or missing.”</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "Values from 1 to 87 are valid counts, while 88 and 99 represent coded responses that need to be recoded for analysis.  \n",
    "To make this variable easier to work with, we replaced code <b>88</b> with <b>0</b> (no children) and treated <b>99</b> and blank responses as missing (<b>NaN</b>).\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "This keeps the variable numeric and ready for descriptive statistics or modeling while clearly separating missing or refused answers from true zeros.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724fb8b-bba1-4d53-81de-fbae39101d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_children = df[\"CHILDREN\"][(df[\"CHILDREN\"] >= 1) & (df[\"CHILDREN\"] <= 87)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278aaa3f-e0ce-4527-a982-0d69029b2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total unique valid answers (1–87):\", valid_children.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d33d3-b836-4054-be5e-c08e7e589bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_counts = valid_children.value_counts().sort_index()\n",
    "print(valid_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead89fab-17a2-446e-8e05-b19b9b094c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the CHILDREN column\n",
    "child = df[\"CHILDREN\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e97332-70c6-48d6-8f75-861b08fb25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 88 with 0 (no children), 99 and blanks with NaN\n",
    "child = child.replace({88: 0, 99: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52853f6-5703-4bdd-828d-a40dc415427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace extreme values (more than 20 children) with NaN\n",
    "child = child.mask(child > 20, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530340c-b40c-48c2-8a78-d14211c408b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned version\n",
    "df[\"CHILDREN\"] = child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c6ea9-4615-4355-b84f-38c4acf55083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save value before imputation\n",
    "children_missing_before_imputation = df[\"CHILDREN\"].isnull().sum()\n",
    "print(\"Remaining missing values:\", df[\"CHILDREN\"].isna().sum())\n",
    "print(\"Valid CHILDREN range:\", df[\"CHILDREN\"].min(), \"to\", df[\"CHILDREN\"].max())\n",
    "print(df[\"CHILDREN\"].value_counts().sort_index().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe966e-bbeb-41c7-a150-a39d7602ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix floating point artifact\n",
    "df[\"CHILDREN\"] = df[\"CHILDREN\"].replace(5.397605e-79, 0)\n",
    "\n",
    "# Quick verification\n",
    "print(\"Remaining missing:\", df[\"CHILDREN\"].isna().sum())\n",
    "print(\"Range after final cleaning:\", df[\"CHILDREN\"].min(), \"to\", df[\"CHILDREN\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24ec5d-ed4f-448b-9fda-7b7b64fa19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy with labels\n",
    "df[\"Children_Label\"] = df[\"CHILDREN\"].copy()\n",
    "\n",
    "# Replace NaN with \"Unknown\" for display\n",
    "df[\"Children_Label\"] = df[\"Children_Label\"].fillna(\"Unknown\")\n",
    "\n",
    "# Check result\n",
    "print(df[\"Children_Label\"].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7d975-96a1-4387-a96a-c51c300d20e7",
   "metadata": {},
   "source": [
    "Values greater than <b>20</b> were considered unrealistic and also treated as missing.  \n",
    "A small floating-point artifact (<b>5.397605e-79</b>) was corrected to zero.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "After cleaning, the valid range was <b>0–20 children</b>, and about <b>2%</b> of entries remained missing.  \n",
    "Since missing responses are minimal and likely non-random, no imputation was applied.  \n",
    "This keeps the distribution realistic and avoids introducing artificial bias.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648c853-542f-41d3-b140-099356478a00",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">5.5 Imputing Missing values for EMPLOY1</h2>\n",
    "<p> The variable <b>EMPLOY1</b> records each respondent’s current employment status, using coded values from 1 to 9. According to the BRFSS data dictionary: </p> <ul> <li><b>1</b> – Employed for wages</li> <li><b>2</b> – Self-employed</li> <li><b>3</b> – Out of work for 1 year or more</li> <li><b>4</b> – Out of work for less than 1 year</li> <li><b>5</b> – A homemaker</li> <li><b>6</b> – A student</li> <li><b>7</b> – Retired</li> <li><b>8</b> – Unable to work</li> <li><b>9</b> – Refused (nonresponse)</li> <li><b>Blank</b> – Not asked or missing</li> </ul> <p> To ensure accurate analysis, the invalid and nonresponse codes (<b>9</b> and blanks) were replaced with <b>NaN</b>. A new labeled categorical variable, <b>Employment_Status</b>, was then created using clear text labels for readability. Missing values (i.e., “Refused” or blank responses) were recoded as <b>“Unknown”</b> to preserve respondent counts while distinguishing them from valid categories. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96d22a-fd1c-498c-a72a-c852463df3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Copy the column\n",
    "emp = df[\"EMPLOY1\"].copy()\n",
    "\n",
    "# STEP 2: Replace invalid code (9 = Refused) with NaN\n",
    "emp = emp.mask(emp == 9, np.nan)\n",
    "\n",
    "# STEP 3: Save the cleaned numeric column\n",
    "df[\"EMPLOY1\"] = emp\n",
    "\n",
    "# Save value before imputation\n",
    "employ1_missing_before_imputation = df[\"EMPLOY1\"].isnull().sum()\n",
    "\n",
    "# Quick check\n",
    "print(\"Unique valid values after cleaning:\", sorted(df[\"EMPLOY1\"].dropna().unique()))\n",
    "print(\"Missing EMPLOY1 values:\", df[\"EMPLOY1\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a8a72-486f-4733-838b-cfcaaa6b5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Define employment status labels\n",
    "employ_labels = {\n",
    "    1: \"Employed for wages\",\n",
    "    2: \"Self-employed\",\n",
    "    3: \"Out of work for 1 year or more\",\n",
    "    4: \"Out of work for less than 1 year\",\n",
    "    5: \"Homemaker\",\n",
    "    6: \"Student\",\n",
    "    7: \"Retired\",\n",
    "    8: \"Unable to work\"\n",
    "}\n",
    "\n",
    "# STEP 5: Create readable categorical column\n",
    "df[\"Employment_Status\"] = df[\"EMPLOY1\"].map(employ_labels).fillna(\"Unknown\")\n",
    "\n",
    "# STEP 6: Verify distribution\n",
    "print(df[\"Employment_Status\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037febf-4b5e-4236-9e10-14125815b054",
   "metadata": {},
   "source": [
    "Out of <b>457,634</b> respondents, the largest groups are <b>Employed for wages</b> (≈<b>40.7%</b>) and <b>Retired</b> (≈<b>32.1%</b>), showing a mix of active workers and retirees.  \n",
    "Smaller segments include <b>Self-employed</b> (≈<b>8.6%</b>), <b>Unable to work</b> (≈<b>6.1%</b>), <b>Homemaker</b> (≈<b>3.9%</b>), and <b>Student</b> (≈<b>2.5%</b>).  \n",
    "Unemployment is relatively low: <b>Out of work &lt; 1 year</b> (≈<b>2.4%</b>) and <b>≥ 1 year</b> (≈<b>2.0%</b>).  \n",
    "Nonresponses are minimal, with <b>Unknown</b> at ≈<b>1.8%</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b56a0-a49f-4b6a-b94f-29bb7666f2da",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">5.6 Summary of Cleaned Variables</h2>\n",
    "\n",
    "<p>\n",
    "The <b>HEIGHT3</b> variable contained self-reported height values, with unrealistic or metric entries replaced by the <b>median height by sex</b>.  \n",
    "This approach ensured a balanced and realistic height distribution between male and female respondents.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "For <b>WEIGHT2</b>, most responses were within a healthy adult range (100–300 lbs).  \n",
    "Unrealistic outliers and invalid codes were replaced with <b>median weights by sex</b>, helping maintain accuracy and consistency in the data.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The <b>INCOME3</b> variable was cleaned and recoded into clear income brackets for easier interpretation.  \n",
    "Approximately <b>19%</b> of responses were missing or uncertain, which were labeled as <b>“Unknown”</b> to preserve respondent counts without introducing bias.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The <b>CHILDREN</b> variable captured the number of people under 18 in each household.  \n",
    "Responses above 20 were treated as invalid and replaced with missing values, while “Refused” and blank entries were labeled as <b>“Unknown”</b> to keep the dataset complete.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Finally, <b>EMPLOY1</b> described each respondent’s employment status.  \n",
    "After cleaning, most respondents were classified as <b>“Employed for wages”</b> or <b>“Retired”</b>, with smaller groups representing self-employed, students, or those unable to work.  \n",
    "Nonresponses were recorded as <b>“Unknown”</b> to maintain consistency with other demographic variables.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cabe58",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\">Step 6 – Checking for inconsistencies</h1>\n",
    "\n",
    "<p>\n",
    "In this step, the dataset will be examined to identify possible inconsistencies or implausible values. Numeric fields will be checked to ensure that ranges and units are logically consistent. Any detected issues will be addressed and documented to preserve data accuracy.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab315e",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">6.1 Identifying numeric columns</h2>\n",
    "\n",
    "<p>\n",
    "This section performs an exploratory scan of all numeric variables, computing basic statistics and identifying extreme values based on the 1st–99th quantile range. The goal is to flag potential anomalies before applying logical range validation in later steps.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f04d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# STEP 6.1 — Extended numeric inconsistency scan (detection only)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Identify numeric columns (post-cleaning) ---\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "n_rows = len(df)\n",
    "\n",
    "print(f\"[Step 6.2] Scanning {len(num_cols)} numeric columns on {n_rows} rows...\")\n",
    "\n",
    "records = []\n",
    "for col in num_cols:\n",
    "    s = df[col]\n",
    "\n",
    "    # --- Basic stats (ignore inf/-inf) ---\n",
    "    s_valid = s.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    q01 = s_valid.quantile(0.01)\n",
    "    q99 = s_valid.quantile(0.99)\n",
    "    col_min = s_valid.min()\n",
    "    col_max = s_valid.max()\n",
    "    n_missing = int(s_valid.isna().sum())\n",
    "\n",
    "    # --- Proposed logical envelope (quantile-based, detection only) ---\n",
    "    # This is a generic flag to surface extreme values for review.\n",
    "    # Domain-specific ranges will be applied in the next sub-steps.\n",
    "    in_bounds = s_valid.between(q01, q99)\n",
    "    n_oob = int((~in_bounds & s_valid.notna()).sum())\n",
    "    pct_oob = (n_oob / n_rows) * 100 if n_rows else 0.0\n",
    "\n",
    "    # --- Extra quick checks ---\n",
    "    n_negative = int((s_valid < 0).sum())\n",
    "\n",
    "    records.append({\n",
    "        \"variable\": col,\n",
    "        \"dtype\": str(s.dtype),\n",
    "        \"min\": col_min,\n",
    "        \"q01\": q01,\n",
    "        \"q99\": q99,\n",
    "        \"max\": col_max,\n",
    "        \"n_missing\": n_missing,\n",
    "        \"n_outside_q01_q99\": n_oob,\n",
    "        \"pct_outside_q01_q99\": round(pct_oob, 4),\n",
    "        \"n_negative\": n_negative\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca2fc1",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">6.2 Displaying top 15 variables outside of logical bounds</h2>\n",
    "\n",
    "<p>\n",
    "This results highlight numeric variables with higher proportions of values outside the 1st–99th percentile range. These results provide an exploratory overview to guide which variables may require further logical validation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddced94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# STEP 6.2 — Summary table sorted by percent outside logical bounds\n",
    "# -----------------------------------------------------------\n",
    "df_inconsistency_allnum = (\n",
    "    pd.DataFrame.from_records(records)\n",
    "      .sort_values(by=[\"pct_outside_q01_q99\", \"n_outside_q01_q99\"], ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"[Step 6.2] Extended numeric scan completed.\")\n",
    "print(\"Top 15 variables by % outside [q01, q99]:\")\n",
    "display_cols = [\n",
    "    \"variable\", \"dtype\", \"min\", \"q01\", \"q99\", \"max\",\n",
    "    \"n_missing\", \"n_negative\", \"n_outside_q01_q99\", \"pct_outside_q01_q99\"\n",
    "]\n",
    "df_inconsistency_allnum[display_cols].head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0ff25",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">6.3 Corrections & unit standardization</h2>\n",
    "\n",
    "<p>\n",
    "In this step, the detected inconsistencies were addressed by creating adjusted versions of key demographic and anthropometric variables. Height and weight were consolidated from multiple sources and converted into consistent units, while implausible values were replaced with missing entries. Age and number of children were also validated against logical population ranges. All adjustments were stored in new <b>*_adj</b> columns to preserve the integrity of the original data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# STEP 6.3 — Corrections & unit standardization (apply changes)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Helpers ---\n",
    "def first_non_null(*series):\n",
    "    \"\"\"Return the first non-null value across multiple aligned Series.\"\"\"\n",
    "    out = series[0].copy()\n",
    "    for s in series[1:]:\n",
    "        out = out.where(out.notna(), s)\n",
    "    return out\n",
    "\n",
    "def to_numeric_safe(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72b482",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">6.3.1 Height (inches)</h2>\n",
    "\n",
    "<p>\n",
    "This block consolidates height information from all available sources, converts centimeters to inches when necessary, and creates an adjusted height variable. Implausible values outside the 48–84 inch range are set to missing to ensure logical consistency.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Height (inches): build HEIGHT3_in_adj using best available source ---\n",
    "h_in  = to_numeric_safe(df.get(\"HEIGHT3_in\")) if \"HEIGHT3_in\" in df.columns else None\n",
    "htin4 = to_numeric_safe(df.get(\"HTIN4\")) if \"HTIN4\" in df.columns else None\n",
    "htm4  = to_numeric_safe(df.get(\"HTM4\")) if \"HTM4\" in df.columns else None  # centimeters\n",
    "\n",
    "height_from_cm = htm4 * 0.3937007874 if htm4 is not None else None\n",
    "height_candidates = [s for s in [h_in, htin4, height_from_cm] if s is not None]\n",
    "\n",
    "if height_candidates:\n",
    "    df[\"HEIGHT3_in_adj\"] = first_non_null(*height_candidates)\n",
    "    # Range enforcement (adult plausible): 48–84 inches\n",
    "    before_valid = df[\"HEIGHT3_in_adj\"].notna().sum()\n",
    "    df.loc[~df[\"HEIGHT3_in_adj\"].between(48, 84), \"HEIGHT3_in_adj\"] = np.nan\n",
    "    after_valid = df[\"HEIGHT3_in_adj\"].notna().sum()\n",
    "    print(f\"[Step 6.3] HEIGHT3_in_adj: valid before={before_valid}, after={after_valid}\")\n",
    "else:\n",
    "    print(\"[Step 6.3] HEIGHT3_in_adj: no height sources available (skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06088df0",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">6.3.2 Weight (pounds)</h2>\n",
    "\n",
    "<p>\n",
    "Weight information is consolidated from pounds and kilograms, converting kilograms to pounds when available and selecting the first non-missing value across sources. Implausible adult weights outside the 70–700 lb range are corrected when possible or set to missing to maintain logical consistency.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Weight (pounds): build WEIGHT_lb_adj using WEIGHT_lb or WTKG3 ---\n",
    "w_lb   = to_numeric_safe(df.get(\"WEIGHT_lb\")) if \"WEIGHT_lb\" in df.columns else None\n",
    "wtkg3  = to_numeric_safe(df.get(\"WTKG3\")) if \"WTKG3\" in df.columns else None\n",
    "w_from_kg = wtkg3 * 2.2046226218 if wtkg3 is not None else None\n",
    "\n",
    "weight_candidates = []\n",
    "# Prefer existing pounds; if missing or implausible, fallback to kg-converted\n",
    "if w_lb is not None:\n",
    "    weight_candidates.append(w_lb)\n",
    "if w_from_kg is not None:\n",
    "    weight_candidates.append(w_from_kg)\n",
    "\n",
    "if weight_candidates:\n",
    "    base_w = first_non_null(*weight_candidates)\n",
    "    # Try to salvage implausible values using kg source when available\n",
    "    # Plausible adult weight in lb: 70–700\n",
    "    w_adj = base_w.copy()\n",
    "    impl_mask = ~(w_adj.between(70, 700))\n",
    "    if w_from_kg is not None:\n",
    "        # Replace implausible pounds with kg-converted where kg is plausible (30–250 kg)\n",
    "        kg_ok = wtkg3.between(30, 250)\n",
    "        w_adj = np.where(impl_mask & kg_ok, w_from_kg, w_adj)\n",
    "        w_adj = pd.to_numeric(w_adj, errors=\"coerce\")\n",
    "\n",
    "    df[\"WEIGHT_lb_adj\"] = w_adj\n",
    "    before_valid = pd.Series(w_adj).notna().sum()\n",
    "    # Final range enforcement\n",
    "    df.loc[~df[\"WEIGHT_lb_adj\"].between(70, 700), \"WEIGHT_lb_adj\"] = np.nan\n",
    "    after_valid = df[\"WEIGHT_lb_adj\"].notna().sum()\n",
    "    print(f\"[Step 6.3] WEIGHT_lb_adj: valid before={before_valid}, after={after_valid}\")\n",
    "else:\n",
    "    print(\"[Step 6.3] WEIGHT_lb_adj: no weight sources available (skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b4f57",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">6.3.3 Children</h2>\n",
    "\n",
    "<p>\n",
    "The reported number of children is validated by enforcing a logical range of 0–20. Values outside this interval are set to missing, and the adjusted results are stored in a separate column to preserve the original data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaead7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CHILDREN: non-negative integers within a reasonable cap ---\n",
    "if \"CHILDREN\" in df.columns:\n",
    "    ch = to_numeric_safe(df[\"CHILDREN\"])\n",
    "    before_valid = ch.notna().sum()\n",
    "    ch = ch.mask(~ch.between(0, 20))  # set implausible counts to NaN\n",
    "    df[\"CHILDREN_adj\"] = ch\n",
    "    after_valid = df[\"CHILDREN_adj\"].notna().sum()\n",
    "    print(f\"[Step 6.3] CHILDREN_adj: valid before={before_valid}, after={after_valid}\")\n",
    "else:\n",
    "    print(\"[Step 6.3] CHILDREN_adj: column not found (skipped)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217ac07",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">6.3.4 Age</h2>\n",
    "\n",
    "<p>\n",
    "Age is taken from the primary source <b>_AGE80</b> and validated to fall within the adult range of 18–99 years. Any values outside this interval are set to missing, and the cleaned version is stored in a separate adjusted column.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9003b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Age: prefer _AGE80 when available; enforce adult range 18–99 ---\n",
    "age_series = None\n",
    "if \"_AGE80\" in df.columns:\n",
    "    age_series = to_numeric_safe(df[\"_AGE80\"])\n",
    "elif \"_AGE\" in df.columns:\n",
    "    age_series = to_numeric_safe(df[\"_AGE\"])  # fallback if exists\n",
    "\n",
    "if age_series is not None:\n",
    "    before_valid = age_series.notna().sum()\n",
    "    age_series = age_series.mask(~age_series.between(18, 99))\n",
    "    df[\"AGE_adj\"] = age_series\n",
    "    after_valid = df[\"AGE_adj\"].notna().sum()\n",
    "    print(f\"[Step 6.3] AGE_adj: valid before={before_valid}, after={after_valid}\")\n",
    "else:\n",
    "    print(\"[Step 6.3] AGE_adj: no age source available (skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1191f7",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">6.4 Summary of data adjustments</h2>\n",
    "\n",
    "<p>\n",
    "A compact summary is generated to document the number of missing and non-missing values for each adjusted variable. This table provides a clear overview of the impact of the corrections applied during the inconsistency handling process.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba85c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compact summary of adjustments (for documentation) ---\n",
    "summary_cols = [c for c in [\"HEIGHT3_in_adj\", \"WEIGHT_lb_adj\", \"BMI_adj\", \"CHILDREN_adj\", \"AGE_adj\"] if c in df.columns]\n",
    "adj_summary = (\n",
    "    pd.DataFrame({\n",
    "        \"variable\": summary_cols,\n",
    "        \"n_missing\": [df[c].isna().sum() for c in summary_cols],\n",
    "        \"n_non_missing\": [df[c].notna().sum() for c in summary_cols]\n",
    "    })\n",
    "    .assign(rows_total=len(df))\n",
    ")\n",
    "print(\"[Step 6.3] Adjustment summary:\")\n",
    "adj_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f62a9",
   "metadata": {},
   "source": [
    "<p>\n",
    "All numeric fields were first screened using quantile-based diagnostics to identify potential anomalies. Logical validations were then applied to height, weight, age, and number of children, ensuring consistent units and enforcing plausible human ranges. Adjusted variables were created to preserve original data, and a summary table documented the impact of these corrections.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908da57",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\">Step 7 – Aggregate the Data</h1>\n",
    "\n",
    "<p>\n",
    "In this step, we will create derived variables and summarize the data cleaning process. \n",
    "This includes calculating Body Mass Index (BMI), creating categorical variables for income and education levels, \n",
    "and providing a comprehensive summary of rows that were dropped or imputed during the cleaning process.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475d11e",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">7.1 Create Body Mass Index (BMI) Column</h2>\n",
    "\n",
    "<p>\n",
    "Body Mass Index (BMI) is a key health indicator that relates an individual's weight to their height. \n",
    "It is calculated using the formula: <b>BMI = weight (kg) / height (m)2</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba395f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the adjusted variables from Step 6\n",
    "height_col = 'HEIGHT3_in_adj' if 'HEIGHT3_in_adj' in df.columns else 'HEIGHT3_in'\n",
    "weight_col = 'WEIGHT_lb_adj' if 'WEIGHT_lb_adj' in df.columns else 'WEIGHT_lb'\n",
    "\n",
    "print(f\"Using {height_col} and {weight_col} for BMI calculation\")\n",
    "\n",
    "# Convert to metric units and calculate BMI\n",
    "# Weight: pounds to kilograms (divide by 2.205)\n",
    "# Height: inches to meters (multiply by 0.0254)\n",
    "df['weight_kg'] = df[weight_col] / 2.205\n",
    "df['height_m'] = df[height_col] * 0.0254\n",
    "\n",
    "# Calculate BMI = weight(kg) / height(m)²\n",
    "df['BMI'] = df['weight_kg'] / (df['height_m'] ** 2)\n",
    "\n",
    "# Check for valid BMI range (typically 10-80)\n",
    "valid_bmi_mask = df['BMI'].between(10, 80)\n",
    "df.loc[~valid_bmi_mask, 'BMI'] = np.nan\n",
    "\n",
    "print(f\"BMI Statistics:\")\n",
    "print(f\"Valid BMI values: {df['BMI'].notna().sum():,}\")\n",
    "print(f\"Missing BMI values: {df['BMI'].isna().sum():,}\")\n",
    "print(f\"BMI Range: {df['BMI'].min():.1f} to {df['BMI'].max():.1f}\")\n",
    "print(f\"Mean BMI: {df['BMI'].mean():.1f}\")\n",
    "print(f\"Median BMI: {df['BMI'].median():.1f}\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['weight_kg', 'height_m'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f45f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BMI categories according to WHO standards\n",
    "def categorize_bmi(bmi):\n",
    "    if pd.isna(bmi):\n",
    "        return \"Unknown\"\n",
    "    elif bmi < 18.5:\n",
    "        return \"Underweight\"\n",
    "    elif 18.5 <= bmi < 25:\n",
    "        return \"Normal weight\"\n",
    "    elif 25 <= bmi < 30:\n",
    "        return \"Overweight\"\n",
    "    else:\n",
    "        return \"Obese\"\n",
    "\n",
    "df['BMI_Category'] = df['BMI'].apply(categorize_bmi)\n",
    "\n",
    "# Display BMI category distribution\n",
    "print(\"BMI Category Distribution:\")\n",
    "print(df['BMI_Category'].value_counts(dropna=False))\n",
    "print(f\"\\nPercentage Distribution:\")\n",
    "bmi_pct = (df['BMI_Category'].value_counts(dropna=False) / len(df) * 100).round(2)\n",
    "for category, percentage in bmi_pct.items():\n",
    "    print(f\"{category}: {percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df91b5",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">7.2 Create Appropiate Categories for Income and Education Levels</h2>\n",
    "\n",
    "<p>\n",
    "We will create appropiate categorization to create more meaningful groupings for analysis.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f048308",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">7.2.1 Create Income Level Categories</h2>\n",
    "\n",
    "<p>\n",
    "We will create an income categorie based on INCOME3, this will be consolidated into more interpretable groups \n",
    "that facilitate statistical analysis and visualization.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create consolidated income categories\n",
    "def categorize_income(income_code):\n",
    "    if pd.isna(income_code):\n",
    "        return \"Unknown\"\n",
    "    elif income_code in [1, 2, 3]:  # Less than $20,000\n",
    "        return \"Low Income\"\n",
    "    elif income_code in [4, 5]:     # $20,000 to < $35,000\n",
    "        return \"Lower-Middle Income\"\n",
    "    elif income_code in [6, 7]:     # $35,000 to < $75,000\n",
    "        return \"Middle Income\"\n",
    "    elif income_code in [8, 9]:     # $75,000 to < $150,000\n",
    "        return \"Upper-Middle Income\"\n",
    "    elif income_code in [10, 11]:   # $150,000 or more\n",
    "        return \"High Income\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply income categorization\n",
    "df['Income_Category'] = df[\"INCOME3\"].apply(categorize_income)\n",
    "\n",
    "# Display income category distribution\n",
    "print(\"Income Category Distribution:\")\n",
    "income_dist = df['Income_Category'].value_counts(dropna=False)\n",
    "print(income_dist)\n",
    "\n",
    "print(f\"\\nPercentage Distribution:\")\n",
    "income_pct = (income_dist / len(df) * 100).round(2)\n",
    "for category, percentage in income_pct.items():\n",
    "    print(f\"{category}: {percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232fd113",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">7.2.2 Create Education Level Categories</h2>\n",
    "\n",
    "<p>\n",
    "Education levels will be categorized into meaningful groups based on the EDUCA variabe.\n",
    "This categorization will help the analysis of health outcomes by educational attainment and \n",
    "provide insights into socioeconomic patterns in the data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create education categories based on standard coding\n",
    "def categorize_education(edu_code):\n",
    "    if pd.isna(edu_code) or edu_code == 9:\n",
    "        return \"Unknown\"\n",
    "    elif edu_code == 1:\n",
    "        return \"No Formal Education\"\n",
    "    elif edu_code == 2:\n",
    "        return \"Elementary\"\n",
    "    elif edu_code == 3:\n",
    "        return \"Some High School\"\n",
    "    elif edu_code == 4:\n",
    "        return \"High School Graduate\"\n",
    "    elif edu_code == 5:\n",
    "        return \"Some College/Technical\"\n",
    "    elif edu_code == 6:\n",
    "        return \"College Graduate\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply education categorization\n",
    "df['Education_Category'] = df[\"EDUCA\"].apply(categorize_education)\n",
    "\n",
    "# Display education category distribution\n",
    "print(\"Education Category Distribution:\")\n",
    "edu_dist = df['Education_Category'].value_counts(dropna=False)\n",
    "print(edu_dist)\n",
    "\n",
    "print(f\"\\nPercentage Distribution:\")\n",
    "edu_pct = (edu_dist / len(df) * 100).round(2)\n",
    "for category, percentage in edu_pct.items():\n",
    "    print(f\"{category}: {percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29847c",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4A460;\">7.3 Summary of Rows Dropped and Imputed</h2>\n",
    "\n",
    "<p>\n",
    "This section provides a  summary of all data cleaning operations performed throughout the analysis,\n",
    "including the number of rows dropped, columns removed, and values imputed.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for original data shape\n",
    "original_rows = 457670\n",
    "original_cols = 301\n",
    "\n",
    "# Variables for transformated data shape\n",
    "current_rows = len(df)\n",
    "current_cols = len(df.columns)\n",
    "\n",
    "print(f\"\\n DATASET OVERVIEW:\")\n",
    "print(f\"{'Original dataset shape:':<30} {original_rows:,} rows × {original_cols} columns\")\n",
    "print(f\"{'Final dataset shape:':<30} {current_rows:,} rows × {current_cols} columns\")\n",
    "print(f\"{'Rows retained:':<30} {current_rows:,} ({(current_rows/original_rows)*100:.1f}%)\")\n",
    "print(f\"{'Columns retained:':<30} {current_cols} ({(current_cols/original_cols)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n ROWS DROPPED:\")\n",
    "rows_dropped = original_rows - current_rows\n",
    "print(f\"{'Total rows dropped:':<30} {rows_dropped:,} ({(rows_dropped/original_rows)*100:.1f}%)\")\n",
    "print(f\"{'- Duplicate rows:':<30} 0 (no duplicates found)\")\n",
    "print(f\"{'- Rows with missing values:':<30} {rows_dropped:,} (negligible missing data)\")\n",
    "\n",
    "print(f\"\\n COLUMNS DROPPED:\")\n",
    "cols_dropped = original_cols - current_cols\n",
    "print(f\"{'Total columns dropped:':<30} {cols_dropped} ({(cols_dropped/original_cols)*100:.1f}%)\")\n",
    "print(f\"{'- Excessive missing data (>15k):':<30} {cols_dropped-60} columns\")\n",
    "print(f\"{'- Irrelevant Columns:':<30} 60 columns\")\n",
    "\n",
    "print(f\"\\n VALUES IMPUTED:\")\n",
    "height3_imputed = height3_missing_before_imputation - df['HEIGHT3'].isna().sum()\n",
    "weight2_imputed = weight2_missing_before_imputation - df['WEIGHT2'].isna().sum()\n",
    "income3_processed = income3_missing_before_imputation - (df['Income_Level'] == 'Unknown').sum()\n",
    "children_processed = children_missing_before_imputation - (df['Children_Label'] == 'Unknown').sum()\n",
    "employ1_processed = employ1_missing_before_imputation - (df['Employment_Status'] == 'Unknown').sum()\n",
    "\n",
    "print(f\"{'HEIGHT3 (median by sex):':<35} {height3_imputed:,} values imputed\")\n",
    "print(f\"{'WEIGHT2 (median by sex):':<35} {weight2_imputed:,} values imputed\")\n",
    "print(f\"{'INCOME3 (set to Unknown):':<35} {income3_processed:,} values processed (labeled as Unknown)\")\n",
    "print(f\"{'CHILDREN (kept as missing):':<35} {children_processed:,} values processed (labeled as Unknown)\")\n",
    "print(f\"{'EMPLOY1 (set to Unknown):':<35} {employ1_processed:,} values processed (labeled as Unknown)\")\n",
    "\n",
    "# Save cleaned dataset to a new CSV file\n",
    "df.to_csv(\"LLCP2024_Cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c36ed-3425-4106-b0dd-ebaf070e408c",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#E67E22;\"> Step - 8 Final Summary of the Project</h1>\n",
    "\n",
    "<p>\n",
    "This section gives a general overview of everything done during the project using the CRISP-DM approach. We kicked things off with a clear idea of our analytical goals, then took a good look at the dataset to get a sense of its structure, quality, and main features. In the Data Understanding and Preparation phase, we spotted important issues like missing values, outliers, and how the categories were distributed, all through descriptive stats and visual aids.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "During the Data Preparation phase, we tackled all the necessary cleaning tasks, like getting rid of unnecessary columns, dealing with duplicates, dropping rows that had a few missing values, and filling in key variables. This ensured our dataset was accurate and ready to use. We also made some additional transformations and recoding to keep everything consistent and prepare it for future modeling.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "By fully understanding and prepping the data, this project lays a dependable foundation for more in-depth statistical or predictive work down the line in the CRISP-DM process.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
